---
title: "sites_scraping"
output: html_document
date: '2022-06-06'
---

```{r setup}
library(tidyverse)
library(purrr)
library(rvest)
library(robotstxt)
library(XML)
library(stringr)
```


##get all wikipedia results from google search
```{r}
df <- tibble(read_csv("./data/results_data.csv"))
wikipedia_df <- df %>% filter(str_detect(link, ".wikipedia."))
wikipedia_df
```



```{r}
new_wikipidia_df <- tibble(
      termID = integer(),
      searchTerm = character(),
      language = character(),
      google_search_position = integer(), 
      link = character(),
      img_num = integer(),
      sections = integer(),
      references = integer(),
      word_count = integer(),
      term_appirance = integer()
    )

```

##gather metadata from wikipedia pages

```{r}
for (i in 1:nrow(wikipedia_df)){
      webPage <- read_html(wikipedia_df[i,]$link)
      img_num <- length(webPage %>% html_nodes(".thumbimage"))
      sections <- length(webPage %>% html_nodes("h2 .mw-headline"))+1
      references <- length(webPage %>% html_nodes("#mw-content-text a"))
      all_text <- str_replace_all(tolower(paste(webPage %>% html_nodes("p+ ul li , #siteSub , #firstHeading , .navigation-not-searchable , p") %>% html_text(), collapse = " ")), "[\r\n]", "")
      term_count <- str_count(all_text, regex(tolower(wikipedia_df[i,]$searchTerm)))
      new_wikipidia_df <- new_wikipidia_df %>% add_row(
          termID = wikipedia_df[i,]$termId,
          searchTerm = wikipedia_df[i,]$searchTerm,
          language = wikipedia_df[i,]$language,
          google_search_position = wikipedia_df[i,]$position, 
          link = wikipedia_df[i,]$link,
          img_num = img_num,
          sections = sections,
          references = references,
          word_count = word_num,
          term_appirance = term_count
        )
    }
new_wikipidia_df
```

```{r}
summary(new_wikipidia_df)
```


```{r}
top3_df <- df %>%
  filter(position < 4) %>% 
  filter(language %in% c("en", "ru", "iw", "ar"))
glimpse(top3_df)
write_csv(top3_df,"./data/top3.csv")
```



















